{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an assignment completed in class, we were tasked with utilizing machine learning on Spark.  We used sentiment analysis to classify if Amazon reviews were positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps    # for the pyspark suite\n",
    "import os               # for environ variables in Part 3\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .appName(\"df lecture\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with loading Amazon reviews from json into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = spark.read.json('../data/reviews_Musical_Instruments_5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_reviews.printSchema() # take a look at our schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10261\n"
     ]
    }
   ],
   "source": [
    "print(df_reviews.count()) # how many observations our dataframe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only interested in the 'reviewText' and 'overall' columns, so let's only use those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus = df_reviews.select('reviewText', 'overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- overall: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_corpus.printSchema() # double check our new schema "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to look at positive or negative reviews, we need to create labels for classification, as well as make sure our classes are balanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratings are 1-5, so let's get a total count of each rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- overall: double (nullable = true)\n",
      " |-- count(overall): long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_test = df_corpus.groupBy(\"overall\").agg(count(\"overall\"))\n",
    "res_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class representation: {1.0: 217, 4.0: 2084, 3.0: 772, 2.0: 250, 5.0: 6938}\n"
     ]
    }
   ],
   "source": [
    "classes_count = dict(res_test.collect())\n",
    "print(\"class representation: {}\".format(classes_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fairly imbalanced classes. We are only looking at positive and negative reviews, so lets look at the extremes (1 and 5). Then we can upsample or downsample to make balanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using limit size: 217\n"
     ]
    }
   ],
   "source": [
    "balanced_classsize = min(classes_count[1.0], classes_count[5.0], 10000)\n",
    "print(\"using limit size: {}\".format(balanced_classsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use 'filter' to create a 'positive' reviews dataframe and a second 'negative' reviews dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_neg = df_corpus.filter(df_corpus[\"overall\"] <= 1.0).orderBy(rand()).limit(balanced_classsize)\n",
    "dataset_pos = df_corpus.filter(df_corpus[\"overall\"] >= 5.0).orderBy(rand()).limit(balanced_classsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then combine these two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posnegdataset = dataset_pos.union(dataset_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirmation of balanced dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data points in the neg class: 217\n",
      "data points in the pos class: 217\n"
     ]
    }
   ],
   "source": [
    "print(\"data points in the neg class: {}\".format(dataset_neg.count()))\n",
    "print(\"data points in the pos class: {}\".format(dataset_pos.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new column with '0' as label for negative class and '1' as label for positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_posnegdataset = df_posnegdataset.withColumn(\"label\", (df_posnegdataset['overall']-1.0)/4.0)\n",
    "df_posnegdataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import 'indexing_pipeline' function created in nlp_pipeline file in folder. Use that function to get features and vocabulary of the dataframe, and to index every review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_pipeline import indexing_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output, vocab = indexing_pipeline(df_posnegdataset, inputCol=\"reviewText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- bow: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- vector_tf: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_output.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at some vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: ['guitar', 'b', 'string', 'pedal', 'great', 'good', 'sound', 'other', 'time', 'product']\n"
     ]
    }
   ],
   "source": [
    "print(\"vocabulary: {}\".format(vocab[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use persist to split our data into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[reviewText: string, overall: double, label: double, bow: array<string>, vector_tf: vector, features: vector]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = df_output.randomSplit([0.7, 0.3])\n",
    "df_train = splits[0]\n",
    "df_test = splits[1]\n",
    "\n",
    "df_train.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the naive bayes ML algorithm for features, labels, and predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "# train the model\n",
    "model = nb.fit(df_train)\n",
    "\n",
    "# apply the model on the test set\n",
    "result = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use multiclass classification evaluator to get accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6746031746031746\n"
     ]
    }
   ],
   "source": [
    "# keep only label and prediction to compute accuracy\n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                                  metricName=\"accuracy\")\n",
    "\n",
    "print(\"Accuracy: {}\".format(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has an approx. 67% chance of predicting if an instrument review will be positive or negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see if we can find words related to the positive class, as well as words related to the negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "thetaarray = model.theta.toArray().T\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "dtype = [('label', 'S10'), ('neg', float), ('pos', float)]\n",
    "prob_values = [ (vocab[i],\n",
    "                 np.exp(thetaarray[i,0])*(1-np.exp(thetaarray[i,1])),\n",
    "                 (1-np.exp(thetaarray[i,0]))*np.exp(thetaarray[i,1]))\n",
    "               for i in range(vocab_size) ]\n",
    "\n",
    "a = np.array(prob_values, dtype=dtype)       # create a structured array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('guitar', 0.006222922580237934, 0.008793205823640094)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(b'pedal', 0.00667606, 0.01036225),\n",
       "       (b'string', 0.00391814, 0.00955271),\n",
       "       (b'guitar', 0.00622292, 0.00879321),\n",
       "       (b'great', 0.00169551, 0.00781073),\n",
       "       (b'ipad', 0.0014542 , 0.00682595),\n",
       "       (b'delay', 0.00035155, 0.00615357),\n",
       "       (b'easi', 0.00063057, 0.00605684),\n",
       "       (b'sound', 0.00373599, 0.00598788),\n",
       "       (b'mike', 0.00040292, 0.0053778 ),\n",
       "       (b'price', 0.0027482 , 0.00501963),\n",
       "       (b'good', 0.00468863, 0.00496378),\n",
       "       (b'pick', 0.00283501, 0.00482706),\n",
       "       (b'other', 0.00448186, 0.004827  ),\n",
       "       (b'cabl', 0.00386692, 0.00481758),\n",
       "       (b'jam', 0.00070566, 0.0047976 ),\n",
       "       (b'stand', 0.00327582, 0.004767  ),\n",
       "       (b'tone', 0.00152812, 0.00466939),\n",
       "       (b\"b'i\", 0.00195458, 0.00439361),\n",
       "       (b'nice', 0.00143984, 0.0043902 ),\n",
       "       (b'tuner', 0.00469819, 0.0043608 )],\n",
       "      dtype=[('label', 'S10'), ('neg', '<f8'), ('pos', '<f8')])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(a, order='pos')[::-1][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(b'pedal', 0.00667606, 0.01036225),\n",
       "       (b'record', 0.00643652, 0.0016922 ),\n",
       "       (b'guitar', 0.00622292, 0.00879321),\n",
       "       (b'product', 0.00610722, 0.00305226),\n",
       "       (b'thing', 0.00518676, 0.00162531),\n",
       "       (b'cheap', 0.00492438, 0.00114616),\n",
       "       (b'mic', 0.00486441, 0.00245868),\n",
       "       (b'tuner', 0.00469819, 0.0043608 ),\n",
       "       (b'good', 0.00468863, 0.00496378),\n",
       "       (b'problem', 0.00460113, 0.0027619 ),\n",
       "       (b'other', 0.00448186, 0.004827  ),\n",
       "       (b'time', 0.00428997, 0.003345  ), (b'b', 0.00425538, 0.00403088),\n",
       "       (b'capo', 0.00422081, 0.00342679),\n",
       "       (b'review', 0.00417728, 0.00135955),\n",
       "       (b'strap', 0.00416568, 0.00412251),\n",
       "       (b'batteri', 0.00413894, 0.00240294),\n",
       "       (b'someth', 0.00412973, 0.0017657 ),\n",
       "       (b'same', 0.00408317, 0.00122643),\n",
       "       (b'way', 0.00403511, 0.00261567)],\n",
       "      dtype=[('label', 'S10'), ('neg', '<f8'), ('pos', '<f8')])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(a, order='neg')[::-1][0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words in the positive class like 'great', 'easy', 'good', and 'nice' are clearly associated with  the positive class.  Words like 'problem' and 'cheap' are more obviously related to the negative class. However, most of the words are more product related. That is because we ran the model on a dataset that only included instrument reviews. Therefore, the positive and negative associations are biased by the vocabulary related to the products that people evaluate as positive or negative. To get a better sense of words associated with positive or negative reviews, we need to broaden our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
