{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate a `SparkSession`. A `SparkSession` initializes both a `SparkContext` and a `SQLContext` to use RDD-based and DataFrame-based functionalities of Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps    # for the pyspark suite\n",
    "import os               # for environ variables in Part 3\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .appName(\"df lecture\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sentiment Analysis using Naive Bayes\n",
    "\n",
    "Here we are going to create a text indexing pipeline for user reviews based on Spark ML library.\n",
    "\n",
    "We propose to work on Amazon Reviews. These reviews, made available by \\[[Julian McAuley, UCSD](http://jmcauley.ucsd.edu/data/amazon/)\\], are raw qualitative (text) and quantitative (rating) evaluations of products by users. We propose to use ML+NLP on positive/negative reviews to see what words carry out a positive/negative meaning for users.\n",
    "\n",
    "Here is the plan:\n",
    "\n",
    "1.1. We will load Amazon Reviews from json into a dataframe.\n",
    "\n",
    "1.2. We will create a `label` (positive/negative) for each review and a dataset for ML.\n",
    "\n",
    "1.3. We will create `features` from a the text indexing pipeline using a TF-IDF approach.\n",
    "\n",
    "1.4. We will use Machine Learning (NaiveBayes classifier) to classify positive/negative reviews.\n",
    "\n",
    "1.5. We will interpret this classification in terms of positive/negative terms.\n",
    "\n",
    "We recommend you to work from a notebook using our previous instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load an Amazon Reviews json file\n",
    "\n",
    "First, we will work on a local json datafile which contains a limited subset of the [Amazon Reviews](http://jmcauley.ucsd.edu/data/amazon/).\n",
    "\n",
    "1\\. Use [`spark.read.json()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json) to create a dataframe that would contain the content of the file `'data/reviews_Musical_Instruments_5.json.gz'`.\n",
    "\n",
    "Check the structure of that dataframe, and the column detected in the json content, by using `.printSchema()`. It should read like :\n",
    "\n",
    "```\n",
    "root\n",
    " |-- asin: string (nullable = true)\n",
    " |-- helpful: array (nullable = true)\n",
    " |    |-- element: long (containsNull = true)\n",
    " |-- overall: double (nullable = true)\n",
    " |-- reviewText: string (nullable = true)\n",
    " |-- reviewTime: string (nullable = true)\n",
    " |-- reviewerID: string (nullable = true)\n",
    " |-- reviewerName: string (nullable = true)\n",
    " |-- summary: string (nullable = true)\n",
    " |-- unixReviewTime: long (nullable = true)\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: file:/Users/ryanholway/Documents/galvanize/dsi-solns-g80/spark-ml/data/reviews_Musical_Instruments_5.json.gz;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.3.2/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.3.2/libexec/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o45.json.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/ryanholway/Documents/galvanize/dsi-solns-g80/spark-ml/data/reviews_Musical_Instruments_5.json.gz;\n\tat org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:719)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:390)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:390)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:389)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:397)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2e9011c9967d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/reviews_Musical_Instruments_5.json.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.3.2/libexec/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.3.2/libexec/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.3.2/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: file:/Users/ryanholway/Documents/galvanize/dsi-solns-g80/spark-ml/data/reviews_Musical_Instruments_5.json.gz;'"
     ]
    }
   ],
   "source": [
    "df_reviews = spark.read.json('data/reviews_Musical_Instruments_5.json.gz')\n",
    "\n",
    "df_reviews.printSchema()\n",
    "\n",
    "print(df_reviews.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. From now on, we will keep only the columns `reviewText` and `overall`. Use `.select()` on the dataframe to keep those two only. You can check your transformation using `.printSchema()` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- overall: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_corpus = df_reviews.select('reviewText', 'overall')\n",
    "\n",
    "df_corpus.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Create a `label` for classification and a balanced dataset\n",
    "\n",
    "This dataset is made of user reviews and ratings:\n",
    "\n",
    "* the `reviewText` column (string) contains the raw text of the review.\n",
    "* the `overall` column (double) contains the rating given by the user, in `{1.0, 2.0, 3.0, 4.0, 5.0}`\n",
    "\n",
    "1\\. Using [`.groupBy()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy) and [`.agg()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.agg), count the number of reviews in each rating value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class representation: {1.0: 217, 2.0: 250, 3.0: 772, 4.0: 2084, 5.0: 6938}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "res_test = df_corpus.groupBy(\"overall\").agg(count(\"overall\"))\n",
    "#res_test.printSchema()\n",
    "\n",
    "classes_count = dict(res_test.collect())\n",
    "print(\"class representation: {}\".format(classes_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. We are going to focus on extreme ratings `{1.0, 5.0}`. Using your count, identify how much examples in each of these two classes we need to keep to build a balanced set of examples having the same number of reviews in `1.0` and `5.0`.\n",
    "\n",
    "**Note**: depending on the ML algorithm you use to classify pos/neg classes, having a balanced dataset can be a pre-requisite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using limit size: 217\n"
     ]
    }
   ],
   "source": [
    "#balanced_classsize = min(classes_count[1.0], classes_count[5.0])\n",
    "balanced_classsize = min(classes_count[1.0], classes_count[5.0], 10000)\n",
    "print(\"using limit size: {}\".format(balanced_classsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. By using [`.filter()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.filter) on your dataframe create two dataframes:\n",
    "\n",
    "* one for the reviews having an `overall` of `1.0` (we will call them the `neg` class),\n",
    "* another for the reviews having an `overall` of `5.0` (we will call them the `pos` class).\n",
    "\n",
    "Limit the number of reviews in each dataframe by the number you have identified previously. Be sure to shuffle those reviews before you apply your limit (you can use [`.orderBy()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy) and [`rand()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.rand) for that).\n",
    "\n",
    "Using [`.union()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.union) between dataframes, create a single dataframe containing the samples from both the balanced `neg` and `pos` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data points in the neg class: 217\n",
      "data points in the pos class: 217\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "dataset_neg = df_corpus.filter(df_corpus[\"overall\"] <= 1.0).orderBy(rand()).limit(balanced_classsize)\n",
    "dataset_pos = df_corpus.filter(df_corpus[\"overall\"] >= 5.0).orderBy(rand()).limit(balanced_classsize)\n",
    "\n",
    "df_posnegdataset = dataset_pos.union(dataset_neg)\n",
    "\n",
    "print(\"data points in the neg class: {}\".format(dataset_neg.count()))\n",
    "print(\"data points in the pos class: {}\".format(dataset_pos.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Using [`.withColumn()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumn) create a new column called `label` that has a value of `0.0` for the `neg` class, and `1.0` for the `pos` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posnegdataset = df_posnegdataset.withColumn(\"label\", (df_posnegdataset['overall']-1.0)/4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Check your dataframe at this step using `.printSchema()`. It should look like:\n",
    "\n",
    "```\n",
    "root\n",
    " |-- reviewText: string (nullable = true)\n",
    " |-- overall: double (nullable = true)\n",
    " |-- label: double (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_posnegdataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Index every document using a text indexation pipeline\n",
    "\n",
    "1\\. In the file `nlp_pipeline.py` you'll find a function `indexing_pipeline` that will provide a full text indexation pipeline.\n",
    "\n",
    "This function takes as an input a DataFrame and an `inputCol=...` argument you can use to specify which field to apply the TFIDF on. It returns two things: first, the same DataFrame with a field `'features'` and second, the vocabulary.\n",
    "\n",
    "Now apply that function to our previous DataFrame to index every review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- bow: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- vector_tf: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "vocabulary: ['guitar', 'b', 'string', 'great', 'pedal', 'other', 'sound', 'good', 'cabl', 'amp']\n"
     ]
    }
   ],
   "source": [
    "from nlp_pipeline import indexing_pipeline\n",
    "\n",
    "df_output, vocab = indexing_pipeline(df_posnegdataset, inputCol=\"reviewText\")\n",
    "\n",
    "df_output.printSchema()\n",
    "\n",
    "print(\"vocabulary: {}\".format(vocab[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Applying the Naive Bayes algorithm for sentiment analysis\n",
    "\n",
    "The basics of ML pipelining in spark relies on building step by step instances of classes drawn from the `pyspark.ml` library. We will now use the following class:\n",
    "\n",
    "* [`pyspark.ml.classification.NaiveBayes`](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.NaiveBayes) : implements the Naive Bayes algorithm\n",
    "\n",
    "Using this requires proceeds this way:\n",
    "\n",
    "1. you create an instance `i` specifying columns on which to operate, plus necessary keyword arguments specific to the class\n",
    "2. you fit the machine learning algorithm `i` on your dataframe with a `i.fit()` method, fitting will return a model `m`\n",
    "3. you apply the model `m` on your dataframe using `m.transform()`\n",
    "\n",
    "Look for the definitions of the [`NaiveBayes`](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.NaiveBayes) class in the spark documentation. ou should :\n",
    "\n",
    "* identify the arguments (input/output + parameters) you need to provide.\n",
    "* identify the right values for these arguments.\n",
    "\n",
    "As a starter, the following table gives you the input/output arguments for each class.\n",
    "\n",
    "| Class | input column(s) argument | output column argument | keyword arguments... |\n",
    "|-------|------------|-------------|----------------------|\n",
    "| [`NaiveBayes`](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.NaiveBayes) | `featuresCol=\"features\"` + `labelCol=\"label\"` | `predictionCol=\"prediction\"` | ? |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Before applying the `NaiveBayes` algorithm we will split our dataset into one training set and one testing set using a random split of 70/30. Use [`.randomSplit()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit) to create two distinct dataframes for each of those sets.\n",
    "\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.sampleBy\n",
    "\n",
    "**Note**: You can use `.persist()` to create a persistent training set before applying `NaiveBayes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[reviewText: string, overall: double, label: double, bow: array<string>, vector_tf: vector, features: vector]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = df_output.randomSplit([0.7, 0.3])\n",
    "df_train = splits[0]\n",
    "df_test = splits[1]\n",
    "\n",
    "df_train.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Implement `NaiveBayes` specifying the columns for features (`featureCol`), labels (`labelCol`) and prediction (`predictionCol`). Then `.fit()` to obtain a model, and apply this model on the testing test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "# train the model\n",
    "model = nb.fit(df_train)\n",
    "\n",
    "# apply the model on the test set\n",
    "result = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Use the [`MulticlassClassificationEvaluator`](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator) to obtain an evaluation of the accuracy of your classification.\n",
    "\n",
    "As any other brick in your pipeline, `MulticlassClassificationEvaluator` needs to have columns specified, and some other arguments you need to identify in the documentation. Then, you will need to apply your instance on the prediction and label columns, by using `.evaluate()`. It will compute accuracy (or any other given metric) based on the differences observed between these two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6984126984126984\n"
     ]
    }
   ],
   "source": [
    "# keep only label and prediction to compute accuracy\n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                                  metricName=\"accuracy\")\n",
    "\n",
    "print(\"Accuracy: {}\".format(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Interpretation of the NaiveBayes results\n",
    "\n",
    "The `NaiveBayes` model provides an internal matrix `model.theta` that you can convert into a numpy array with `model.theta.toArray()`. This matrix contains two columns corresponding to the two classes: `0` for `neg` and `1` for `pos`.\n",
    "\n",
    "The values inside that matrix correspond, for each class, to the prior probabilities used to compute the likelihood of a document to belong to the class. In this implementation, the `model.theta` matrix doesn't provide probabilities, but `log` of probabilities.\n",
    "\n",
    "Use this `model.theta` matrix, combined with the vocabulary obtained on question 1.3.4 from `CountVectorizer`, to obtain words that are related to the `pos` class, and words that are related to the `neg` class.\n",
    "\n",
    "Rank these words by their decreasing prior probabilities. What do you see ? How could you enhance these results ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "thetaarray = model.theta.toArray().T\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "dtype = [('label', 'S10'), ('neg', float), ('pos', float)]\n",
    "prob_values = [ (vocab[i],\n",
    "                 np.exp(thetaarray[i,0])*(1-np.exp(thetaarray[i,1])),\n",
    "                 (1-np.exp(thetaarray[i,0]))*np.exp(thetaarray[i,1]))\n",
    "               for i in range(vocab_size) ]\n",
    "\n",
    "a = np.array(prob_values, dtype=dtype)       # create a structured array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('guitar', 0.0057560313686451899, 0.0093975749182296509)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(b'guitar', 0.00575603136864519, 0.00939757491822965),\n",
       "       (b'great', 0.0016933786685190068, 0.008658582256422449),\n",
       "       (b'cabl', 0.0033789793806702118, 0.008192417583327935),\n",
       "       (b'amp', 0.0034001569730091412, 0.007834345684354128),\n",
       "       (b'pedal', 0.0047348306556004695, 0.00776656199005429),\n",
       "       (b'string', 0.005059401122669099, 0.0067799383753098024),\n",
       "       (b'other', 0.004078548851109216, 0.006072426550338713),\n",
       "       (b'good', 0.004351915486973183, 0.005986726803388118),\n",
       "       (b'qualiti', 0.0031672915078594216, 0.005845371284523231),\n",
       "       (b'sound', 0.003607399814728667, 0.005835045552755042),\n",
       "       (b'easi', 0.0007085111643986221, 0.005828266978349922),\n",
       "       (b'stand', 0.0051644450837654374, 0.005647894326469506),\n",
       "       (b'time', 0.004746183013438898, 0.00548092191410527),\n",
       "       (b'differ', 0.002280521322310283, 0.005014254214846147),\n",
       "       (b\"b'i\", 0.002037761710298428, 0.004936646677319166),\n",
       "       (b'price', 0.0034176650400916486, 0.004901009483012242),\n",
       "       (b'strap', 0.005285362561367388, 0.004869161318960482),\n",
       "       (b'more', 0.002947604541394517, 0.0047153149598173376),\n",
       "       (b'set', 0.001984255638992809, 0.004577581185441441),\n",
       "       (b'littl', 0.0013296757311906761, 0.004559988386485883)], \n",
       "      dtype=[('label', 'S10'), ('neg', '<f8'), ('pos', '<f8')])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(a, order='pos')[::-1][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(b'mic', 0.006599920443538227, 0.0014076421162045943),\n",
       "       (b'product', 0.005779401283401342, 0.0037354448339636218),\n",
       "       (b'guitar', 0.00575603136864519, 0.00939757491822965),\n",
       "       (b'strap', 0.005285362561367388, 0.004869161318960482),\n",
       "       (b'stand', 0.0051644450837654374, 0.005647894326469506),\n",
       "       (b'string', 0.005059401122669099, 0.0067799383753098024),\n",
       "       (b'cheap', 0.00495551514257463, 0.0012895787279319135),\n",
       "       (b'time', 0.004746183013438898, 0.00548092191410527),\n",
       "       (b'pedal', 0.0047348306556004695, 0.00776656199005429),\n",
       "       (b'batteri', 0.0045648173348337975, 0.0004574806806342569),\n",
       "       (b'tuner', 0.004528548144069985, 0.0023176168678221525),\n",
       "       (b'thing', 0.004489194923372196, 0.0019119558601155317),\n",
       "       (b'seller', 0.004420166163973924, 9.58665087677645e-05),\n",
       "       (b'good', 0.004351915486973183, 0.005986726803388118),\n",
       "       (b'item', 0.004309878029690998, 0.0007066076349042201),\n",
       "       (b'b', 0.004181699633989091, 0.004317036535713622),\n",
       "       (b'i', 0.004181436996377656, 0.002632969505546132),\n",
       "       (b'other', 0.004078548851109216, 0.006072426550338713),\n",
       "       (b'record', 0.00401131165801634, 0.0024980607725586085),\n",
       "       (b'way', 0.0040104062560362705, 0.002296925732555373)], \n",
       "      dtype=[('label', 'S10'), ('neg', '<f8'), ('pos', '<f8')])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(a, order='neg')[::-1][0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We can observe some words that clearly carry out a positive/negative feeling. But they are mixed with other words that are only related to the products. It's because we have run this analysis on a dataset based on Instruments only. Thus, the positive/negative concept it biased by the terms related to the products people generally evaluate positively (or negatively)._"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
